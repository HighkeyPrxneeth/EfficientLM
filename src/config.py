# from dataclasses import dataclass

# @dataclass
# class Config:
#     vocab_size: int = 50257  # gpt-2 tokenizer
#     block_size: int = 1024   # max context length
#     n_layer: int = 12
#     n_head: int = 12
#     n_embd: int = 768
#     hidden_dim: int = 1024
#     eos_token_id: int = vocab_size - 1  # typically the last token

